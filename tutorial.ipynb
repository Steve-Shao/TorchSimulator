{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This tutorial demonstrates how to create a simulation class by inheriting from the `CTMDPSimulator` class. \n",
    "\n",
    "The example is a dynamic scheduling problem for a call center system. It is from the paper:\n",
    "- [Dynamic Scheduling of a Multiclass Queue in the Halfin-Whitt Regime: A Computational Approach for High-Dimensional Problems](https://arxiv.org/abs/2407.08818), \n",
    "\n",
    "authored by [Baris Ata](https://www.chicagobooth.edu/faculty/directory/a/baris-ata) and [Ebru Kasilarlar](https://ekasikaralar.github.io/Ebru-Kasikaralar/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Call Center Scheduling Problem**\n",
    "\n",
    "In this example, we will simulate the scheduling of agents in a multi-class call center to minimize total costs within a finite time horizon $[0, T]$.\n",
    "\n",
    "### System Characteristics\n",
    "\n",
    "- **Caller Classes $S$**: Multiple classes of callers, each with specific attributes.\n",
    "- **Arrival Rates $\\lambda_s(t)$**: Time-varying arrival rates for each caller class $s \\in \\{1, \\dots, S\\}$.\n",
    "- **Service Rates $\\mu_s$**: Service rates for each caller class $s$.\n",
    "- **Abandonment Rates $\\theta_s$**: Rates at which callers abandon the queue for each class $s$.\n",
    "- **Costs**:\n",
    "  - **Holding Costs $c_s$**: Cost per waiting caller of class $s$.\n",
    "  - **Abandonment Costs $p_s$**: Cost incurred when a caller of class $s$ abandons.\n",
    "  - **Overtime Costs $\\bar{c}$**: Cost for exceeding the planning horizon.\n",
    "- **Agents $N(t)$**: A single pool of homogeneous agents available at time $t$.\n",
    "- **Planning Horizon**: A finite time period $[0, T]$ for scheduling.\n",
    "\n",
    "### State Representation\n",
    "\n",
    "The state of the system at time $t$ is represented by:\n",
    "$$\n",
    "X(t) = (X_1(t), X_2(t), \\dots, X_S(t))\n",
    "$$\n",
    "where $X_s(t)$ denotes the number of waiting callers of class $s$ at time $t$.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action at time $t$ is:\n",
    "$$\n",
    "\\psi(t) = (\\psi_1(t), \\psi_2(t), \\dots, \\psi_S(t))\n",
    "$$\n",
    "where $\\psi_s(t)$ is the number of callers of class $s$ being served at time $t$.\n",
    "\n",
    "**Constraints on Actions**: \n",
    "\n",
    "1. **Capacity Constraint**:\n",
    "   $$\n",
    "   \\sum_{s=1}^S \\psi_s(t) \\leq N(t)\n",
    "   $$\n",
    "2. **Non-negativity and Service Limits**:\n",
    "   $$\n",
    "   0 \\leq \\psi_s(t) \\leq X_s(t) \\quad \\forall s \\in \\{1, \\dots, S\\}\n",
    "   $$\n",
    "\n",
    "### Transition Rates\n",
    "\n",
    "The system evolves as a Continuous-Time Markov Decision Process (CTMDP) with three types of events for each class $s$:\n",
    "\n",
    "1. **Arrivals**:\n",
    "   $$\n",
    "   \\text{Rate} = \\lambda_s(t)\n",
    "   $$\n",
    "2. **Service Completions**:\n",
    "   $$\n",
    "   \\text{Rate} = \\mu_s \\psi_s(t)\n",
    "   $$\n",
    "3. **Abandonments**:\n",
    "   $$\n",
    "   \\text{Rate} = \\theta_s X_s(t)\n",
    "   $$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The objective is to minimize the total expected cost over the planning horizon $[0, T]$, which includes holding costs, abandonment costs, and overtime costs.\n",
    "\n",
    "**Instantaneous Cost**:\n",
    "\n",
    "At any time $t$, the instantaneous cost is:\n",
    "$$\n",
    "C(t) = \\sum_{s=1}^S c_s (X_s(t) - \\psi_s(t))\n",
    "$$\n",
    "\n",
    "**Terminal Cost**:\n",
    "\n",
    "At the end of the planning horizon, an overtime cost is incurred if the number of callers exceeds the number of available agents:\n",
    "$$\n",
    "g(X(T)) = \\bar{c} \\cdot \\left( \\sum_{s=1}^S X_s(T) - N(T) \\right)^+\n",
    "$$\n",
    "where $(x)^+ = \\max(x, 0)$.\n",
    "\n",
    "**Total Cost**:\n",
    "\n",
    "The total cost to be minimized is:\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\int_{0}^{T} C(t) \\, dt + g(X(T)) \\right]\n",
    "$$\n",
    "\n",
    "### Control Policies\n",
    "\n",
    "Develop a policy $\\pi$ that dynamically assigns agents to callers based on the current state $X(t)$. The policy determines the action $\\psi(t)$ to minimize the total cost.\n",
    "\n",
    "**Example Policy: $ \\frac{c_s \\mu_s}{\\theta_s} $ Rule**\n",
    "\n",
    "A commonly used priority-based control policy is the $ \\frac{c_s \\mu_s}{\\theta_s} $ rule, which assigns priorities to caller classes based on the ratio of holding cost and service rate to the abandonment rate.\n",
    "\n",
    "1. **Priority Index**:\n",
    "   $$\n",
    "   \\kappa_s = \\frac{c_s \\mu_s}{\\theta_s}\n",
    "   $$\n",
    "2. **Ordering**:\n",
    "   Sort the classes in decreasing order of $\\kappa_s$. Classes with higher $\\kappa_s$ are given higher priority.\n",
    "3. **Action Selection**:\n",
    "   Assign available agents to serve callers starting from the highest priority class, ensuring that:\n",
    "   $$\n",
    "   \\psi_s(t) = \\min(X_s(t), N(t) - \\sum_{k < s} \\psi_k(t))\n",
    "   $$\n",
    "   where the sum is over all classes with higher priority than $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "\n",
    "from simulator.ctmdp_base import CTMDPSimulator  \n",
    "\n",
    "\n",
    "class CallCenterSimulator(CTMDPSimulator):\n",
    "    \"\"\"\n",
    "    A CTMDP simulator for a multi-class call center problem with:\n",
    "      - S classes of callers,\n",
    "      - Time-varying arrival rates (lambda_s(t)),\n",
    "      - Service rates (mu_s),\n",
    "      - Abandonment rates (theta_s),\n",
    "      - Cost per waiting caller c_s,\n",
    "      - A finite planning horizon [0, T],\n",
    "      - A single pool of homogeneous agents, N(t),\n",
    "      - A terminal (overtime) cost g(x) = bar_c * (1^T x - N(T))^+.\n",
    "\n",
    "    The state is X(t) = (X_1(t), ..., X_S(t)), where X_s(t) is the number of\n",
    "    waiting callers of class s at time t.\n",
    "\n",
    "    The action is psi(t) = (psi_1(t), ..., psi_K(t)), where psi_k(t) is the number\n",
    "    of callers of class s being served at time t. The constraints are:\n",
    "      psi_s(t) <= X_s(t)\n",
    "      1^T psi(t) = min(1^T X(t), N(t))\n",
    "\n",
    "    The (instantaneous) cost at time t is \n",
    "        c^top [X(t) - psi(t)], \n",
    "    i.e. the sum of c_s * (X_s(t) - psi_s(t)) over s.\n",
    "\n",
    "    There are 3S event types (arrival, service completion, abandonment for each class).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Dict[str, Any],\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        accuracy: int = 32,\n",
    "        num_paths: int = 10,\n",
    "        seed: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the call center CTMDP simulator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "            - \"S\" : int\n",
    "                The number of classes.\n",
    "            - \"T\" : float\n",
    "                The time horizon.\n",
    "            - \"lambda_5min\" : List[float]\n",
    "                A list of length S for the arrival rates at 5-minute intervals.\n",
    "            - \"mu_hourly\" : List[float]\n",
    "                A list of length S for service rates (mu_s).\n",
    "            - \"theta_hourly\" : List[float]\n",
    "                A list of length S for abandonment rates (theta_s).\n",
    "            - \"arr_cdf\" : List[float]\n",
    "                A list of length S for the cumulative distribution function (CDF) of the conditional probability of arrival going to each class.\n",
    "            - \"cost_holding_hourly\" : List[float]\n",
    "                A list of length S for the holding cost rates (h_s).\n",
    "            - \"cost_abandonment\" : List[float]\n",
    "                A list of length S for the abandonment cost rates (p_s).\n",
    "            - \"cost_total_hourly\" : List[float]\n",
    "                A list of length S for the total cost rates (h_s + theta_s * p_s).\n",
    "            - \"num_server\" : List[float]\n",
    "                A list of length S for the number of available agents at time t.\n",
    "            - \"bar_c\" : float\n",
    "                The overtime cost coefficient for the terminal cost function.\n",
    "            - \"num_state_variables\": int\n",
    "                Should be = S (the dimension of X(t)).\n",
    "        num_paths : int, optional\n",
    "            Number of sample paths run in parallel, by default 10.\n",
    "        device : torch.device, optional\n",
    "            Device to use (CPU or GPU), by default torch.device(\"cpu\").\n",
    "        seed : int, optional\n",
    "            Random seed for reproducibility, by default None.\n",
    "        \"\"\"\n",
    "        # Call parent constructor first\n",
    "        super().__init__(config=config, device=device, accuracy=accuracy, num_paths=num_paths, seed=seed)\n",
    "\n",
    "        # Explicitly set dtypes based on accuracy parameter\n",
    "        self.dtype_float = torch.float32 if accuracy == 32 else torch.float64\n",
    "        self.dtype_int = torch.int32  # Using int32 since we don't need int64 for this simulation\n",
    "        \n",
    "        # ========== Overwrite related variables ========== #\n",
    "        self.S = self.config.get(\"num_state_variables\", 1)\n",
    "        self.E = 3\n",
    "        self.A = self.S\n",
    "        \n",
    "        self.current_states = torch.zeros(\n",
    "            (self.S, self.num_paths), \n",
    "            dtype=self.dtype_int,  # Using int32 for discrete states\n",
    "            device=self.device,\n",
    "            requires_grad=False\n",
    "        )        \n",
    "        self.waiting_customers = torch.zeros(\n",
    "            (self.S, self.num_paths), \n",
    "            dtype=self.dtype_int,\n",
    "            device=self.device,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.next_rates = torch.ones(\n",
    "            (self.E, self.num_paths), \n",
    "            dtype=self.dtype_float,\n",
    "            device=self.device,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.next_actions = torch.zeros(\n",
    "            (self.A, self.num_paths), \n",
    "            dtype=self.dtype_int,\n",
    "            device=self.device,\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        # ========== Additional placeholder variables ========== #\n",
    "        # The unit of time in this system is hour. \n",
    "        # However, for some reason, the arrival rate is given in 5-minute intervals.\n",
    "        self.hour_to_interval_scaler = 60 / 5\n",
    "        self.current_intervals = torch.zeros(\n",
    "            self.num_paths, \n",
    "            dtype=self.dtype_int,\n",
    "            device=self.device,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.max_interval = self.config.get(\"num_interval\", 204) - 1\n",
    "\n",
    "        # This variable will store the reward obtained at the most recent step\n",
    "        # for each sample path.\n",
    "        self.next_reward_by_class = torch.zeros(\n",
    "            (self.S, self.num_paths), \n",
    "            dtype=self.dtype_float,\n",
    "            device=self.device,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.total_reward_by_class = torch.zeros(\n",
    "            (self.S, self.num_paths), \n",
    "            dtype=self.dtype_float,\n",
    "            device=self.device,\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        # ========== Get data from config ========== #\n",
    "        self.lambda_5min = torch.tensor(self.config[\"lambda_5min\"], dtype=self.dtype_float, device=self.device)\n",
    "        self.lambda_hourly_by_interval = self.lambda_5min * self.hour_to_interval_scaler\n",
    "        self.mu_hourly = torch.tensor(self.config[\"mu_hourly\"], dtype=self.dtype_float, device=self.device)\n",
    "        self.theta_hourly = torch.tensor(self.config[\"theta_hourly\"], dtype=self.dtype_float, device=self.device)\n",
    "        self.arr_cdf = torch.tensor(self.config[\"arr_cdf\"], dtype=self.dtype_float, device=self.device)\n",
    "        self.arr_pdf = torch.diff(self.arr_cdf, dim=1, prepend=torch.zeros(self.arr_cdf.shape[0], 1, dtype=self.dtype_float, device=self.device))\n",
    "        self.cost_holding_hourly = torch.tensor(self.config[\"cost_holding_hourly\"], dtype=self.dtype_float, device=self.device)\n",
    "        self.cost_abandonment = torch.tensor(self.config[\"cost_abandonment\"], dtype=self.dtype_float, device=self.device)\n",
    "        self.cost_total_hourly = torch.tensor(self.config[\"cost_total_hourly\"], dtype=self.dtype_float, device=self.device)\n",
    "        self.num_server = torch.tensor(self.config[\"num_server\"], dtype=self.dtype_int, device=self.device)\n",
    "        self.num_server_init = torch.tensor(self.config[\"num_server_init\"], dtype=self.dtype_int, device=self.device)\n",
    "\n",
    "        # Initialize priority_order for each sample path\n",
    "        self.priority_order = torch.zeros((self.num_paths, self.S), dtype=torch.int64, device=self.device)\n",
    "        self._update_control_rules()\n",
    "\n",
    "    def _update_control_rules(self):\n",
    "        \"\"\"\n",
    "        Update the control rules for the call center system.\n",
    "        In the current implementation, we use constant, pre-emptive, priority-based control rules.\n",
    "        Each sample path has its own priority order based on the policy rule.\n",
    "        \"\"\"\n",
    "        self.mu_theta_diff = torch.sub(self.mu_hourly, self.theta_hourly)\n",
    "        self._zero_float = torch.zeros(1, device=self.device, dtype=self.dtype_float)\n",
    "        self.c_mu_theta = torch.where(\n",
    "            self.theta_hourly != 0,\n",
    "            self.cost_total_hourly * self.mu_hourly / self.theta_hourly,\n",
    "            self._zero_float\n",
    "        )\n",
    "        self.c_mu = torch.mul(self.cost_total_hourly, self.mu_hourly)\n",
    "        self.c_mu_theta_diff = torch.mul(self.cost_total_hourly, self.mu_theta_diff)\n",
    "        \n",
    "        self.policy = self.config.get(\"policy\", \"cost\")\n",
    "        if self.policy == \"cost\":\n",
    "            self.kappa = -1.0 * self.cost_total_hourly\n",
    "        elif self.policy == \"c_mu_theta\":\n",
    "            self.kappa = -1.0 * self.c_mu_theta\n",
    "        elif self.policy == \"c_mu\":\n",
    "            self.kappa = -1.0 * self.c_mu\n",
    "        elif self.policy == \"c_mu_theta_diff\":\n",
    "            self.kappa = -1.0 * self.c_mu_theta_diff\n",
    "        elif self.policy == \"mu_theta_diff\":\n",
    "            self.kappa = -1.0 * self.mu_theta_diff\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid policy: {self.policy}\")\n",
    "        \n",
    "        # Expand priority_order for each sample path\n",
    "        sorted_indices = torch.argsort(self.kappa)\n",
    "        self.priority_order = sorted_indices.unsqueeze(0).repeat(self.num_paths, 1)\n",
    "\n",
    "    def _update_actions(self):\n",
    "        \"\"\"\n",
    "        Compute or retrieve the current action for each sample path, given the current state.\n",
    "        This implements a priority-based queueing discipline where:\n",
    "        1. Customer classes are ordered by priority based on the policy rule (cost, c_mu_theta, etc.)\n",
    "        2. Available agents are assigned to customers in order of class priority\n",
    "        3. Within each class, agents serve as many customers as possible up to:\n",
    "           - The number of waiting customers in that class\n",
    "           - The number of remaining available agents\n",
    "        4. Any remaining agents move on to serve the next priority class\n",
    "        Updates self.next_actions with shape (A, num_paths). \n",
    "        Updates self.waiting_customers with shape (S, num_paths). \n",
    "        \"\"\"\n",
    "        # Update the current intervals to locate data from the 5-minute intervals\n",
    "        self.current_intervals = torch.floor(self.current_times * self.hour_to_interval_scaler).to(self.dtype_int)\n",
    "        # Cap current intervals at max_interval\n",
    "        self.current_intervals = torch.minimum(\n",
    "            self.current_intervals,\n",
    "            torch.tensor(self.max_interval, dtype=self.dtype_int, device=self.device)\n",
    "        )\n",
    "\n",
    "        # Reset the last actions to 0\n",
    "        self.next_actions.zero_()\n",
    "        \n",
    "        # Keep track of remaining agents for each path\n",
    "        remaining_agents = self.num_server[self.current_intervals].clone()\n",
    "        \n",
    "        # Iterate over priority levels\n",
    "        for priority in range(self.S):\n",
    "            # Get class indices for current priority level across all paths\n",
    "            class_idxs = self.priority_order[:, priority]\n",
    "            \n",
    "            # Gather the number of waiting customers for the selected classes\n",
    "            waiting = self.current_states[class_idxs, torch.arange(self.num_paths, device=self.device)]\n",
    "            \n",
    "            # Determine how many callers can be served\n",
    "            callers_served = torch.minimum(waiting, remaining_agents)\n",
    "            \n",
    "            # Update actions\n",
    "            self.next_actions[class_idxs, torch.arange(self.num_paths, device=self.device)] = callers_served\n",
    "            \n",
    "            # Update remaining agents\n",
    "            remaining_agents -= callers_served\n",
    "        \n",
    "        # Update the number of waiting customers\n",
    "        self.waiting_customers = self.current_states - self.next_actions \n",
    "\n",
    "    def _update_transition_rates(self):\n",
    "        \"\"\"\n",
    "        Compute the transition rates for each possible event, given the current state and the chosen action.\n",
    "        Updates self.next_rates with shape (E, num_paths) containing:\n",
    "        - rates[0] = arrival rates for each path\n",
    "        - rates[1] = service completion rates for each path \n",
    "        - rates[2] = abandonment rates for each path\n",
    "        Updates self.next_rates_total with shape (num_paths). \n",
    "        \"\"\"\n",
    "        # Reset rates\n",
    "        self.next_rates.zero_()\n",
    "        \n",
    "        # Event type 0: Arrivals - sum arrival rates across all classes\n",
    "        self.next_rates[0] = self.lambda_hourly_by_interval[self.current_intervals]\n",
    "        # Event type 1: Service completions - sum mu * number in service for each class\n",
    "        self.next_rates[1] = torch.sum(self.mu_hourly[:, None] * self.next_actions, dim=0)\n",
    "        # Event type 2: Abandonments - sum theta * number waiting for each class\n",
    "        self.next_rates[2] = torch.sum(self.theta_hourly[:, None] * self.waiting_customers, dim=0)\n",
    "        # Update total rates\n",
    "        self.next_rates_total = torch.sum(self.next_rates, dim=0)\n",
    "\n",
    "        # Note that we need to ensure all next_rates_total are strictly positive\n",
    "        if not torch.all(self.next_rates_total > 0):\n",
    "            raise ValueError(\"All next_rates_total must be strictly positive.\")\n",
    "\n",
    "    def _update_rewards(self):\n",
    "        \"\"\"\n",
    "        Compute the reward for each sample path, given the current state and action.\n",
    "        Updates self.next_reward with shape (num_paths). \n",
    "        Updates self.total_reward with shape (num_paths). \n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the reward for each sample path by class\n",
    "        self.next_reward_by_class = -1.0 * self.cost_total_hourly[:, None] * self.waiting_customers\n",
    "        self.next_reward_by_class *= self.next_time_increments[None, :]\n",
    "        self.total_reward_by_class += self.next_reward_by_class\n",
    "\n",
    "        self.next_reward = torch.sum(self.next_reward_by_class, dim=0)\n",
    "        self.next_reward = self.next_reward\n",
    "        self.total_reward += self.next_reward\n",
    "\n",
    "    def _update_states(self):\n",
    "        \"\"\"\n",
    "        Update the state based on which event occurred for each sample path.\n",
    "        Updates self.current_states with shape (S, num_paths). \n",
    "        \"\"\"\n",
    "        # Event type 0: Arrivals\n",
    "        arrival_mask = (self.next_event_indices == 0)\n",
    "        # Sample which class arrives based on arrival probabilities\n",
    "        arrival_probs = self.arr_pdf[self.current_intervals[arrival_mask]]\n",
    "        arrival_classes = torch.multinomial(arrival_probs, num_samples=1).squeeze()\n",
    "        # Increment state for the sampled class\n",
    "        self.current_states[arrival_classes, arrival_mask] += 1\n",
    "\n",
    "        # Event type 1: Service completions\n",
    "        service_mask = (self.next_event_indices == 1)\n",
    "        # Sample which class departs based on service rates\n",
    "        departure_rates = (self.mu_hourly[:, None] * self.next_actions[:, service_mask]).t()\n",
    "        departure_probs = departure_rates / (torch.sum(departure_rates, dim=1, keepdim=True) + 1e-10)\n",
    "        departure_class = torch.multinomial(departure_probs, num_samples=1).squeeze()\n",
    "        # Decrement state for the sampled class\n",
    "        self.current_states[departure_class, service_mask] -= 1\n",
    "\n",
    "        # Event type 2: Abandonments\n",
    "        abandon_mask = (self.next_event_indices == 2)\n",
    "        # Sample which class abandons based on waiting customers\n",
    "        abandon_probs = (self.theta_hourly[:, None] * self.waiting_customers[:, abandon_mask]).t()\n",
    "        abandon_probs = abandon_probs / (torch.sum(abandon_probs, dim=1, keepdim=True) + 1e-10)\n",
    "        abandon_class = torch.multinomial(abandon_probs, num_samples=1).squeeze()\n",
    "        # Decrement state for the sampled class\n",
    "        self.current_states[abandon_class, abandon_mask] -= 1\n",
    "\n",
    "    def _summarize(self) -> None:\n",
    "        \"\"\"\n",
    "        Summarize the current batch of states, times, and rewards, \n",
    "        and record them in the history.\n",
    "        \"\"\"\n",
    "        # # Save state, time, and last reward\n",
    "        # self.history.append({\n",
    "        #     \"state\": self.current_states.clone().cpu(),\n",
    "        #     \"time\": self.current_times.clone().cpu(),\n",
    "        #     \"reward\": self.total_reward.clone().cpu()\n",
    "        # })\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation at: 2025-01-26 14:36:04\n",
      "\n",
      "=== Final Results ===\n",
      "Terminal State: ['4.37', '6.28', '4.94', '1.10', '0.71', '0.02', '0.00', '1.27', '0.30', '2.06', '0.00', '0.00', '0.00', '0.00', '0.00', '1.89', '0.00']\n",
      "Terminal Time: 17.00\n",
      "Total Loss: -1038.68\n",
      "Total Loss by Class: ['-0.21', '-94.02', '0.00', '-178.45', '-0.68', '0.00', '0.00', '-157.75', '-111.27', '-448.11', '-8.77', '-76.55', '-4.13', '-40.15', '-1.08', '-46.82', '-0.06']\n",
      "Ending simulation at: 2025-01-26 14:49:37\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Example Usage\n",
    "########################################################\n",
    "# Load system data from CSV files\n",
    "dim = 17\n",
    "data_dir = f\"configs/call_center/config_{dim}dim/\"\n",
    "\n",
    "lambd_5min = pd.read_csv(data_dir + f\"main_test_total_arrivals_partial_5min.csv\", header=None)[0].to_numpy()  # Arrival rates\n",
    "mu_hourly = pd.read_csv(data_dir + f\"mu_hourly_{dim}dim.csv\", header=None)[0].to_numpy()  # Service rates\n",
    "theta_hourly = pd.read_csv(data_dir + f\"theta_hourly_{dim}dim.csv\", header=None)[0].to_numpy()  # Abandonment rates \n",
    "arr_cdf = pd.read_csv(data_dir + f\"cdf_{dim}dim.csv\", header=None, delimiter=\",\").to_numpy() \n",
    "cost_holding_hourly = pd.read_csv(data_dir + f\"hourly_holding_cost_{dim}dim.csv\", header=None)[0].to_numpy() \n",
    "cost_abandonment = pd.read_csv(data_dir + f\"abandonment_cost_{dim}dim.csv\", header=None)[0].to_numpy() \n",
    "cost_total_hourly = pd.read_csv(data_dir + f\"hourly_total_cost_{dim}dim.csv\", header=None)[0].to_numpy() \n",
    "num_server = pd.read_csv(data_dir + f\"main_test_agents.csv\", header=None)[0].to_numpy()  # Arrival rates\n",
    "num_server_init = pd.read_csv(data_dir + f\"initialization_{dim}dim.csv\", header=None)[0].to_numpy() \n",
    "\n",
    "# Model configuration dictionary\n",
    "config = {\n",
    "    \"_comment\": \"Dynamic scheduling config for call center system\",\n",
    "    \"num_state_variables\": dim,\n",
    "    \"policy\": \"c_mu_theta\",\n",
    "    \"num_interval\": 204,\n",
    "    \"lambda_5min\": lambd_5min,\n",
    "    \"mu_hourly\": mu_hourly, \n",
    "    \"theta_hourly\": theta_hourly,\n",
    "    \"arr_cdf\": arr_cdf,\n",
    "    \"cost_holding_hourly\": cost_holding_hourly,\n",
    "    \"cost_abandonment\": cost_abandonment,\n",
    "    \"cost_total_hourly\": cost_total_hourly,\n",
    "    \"num_server\": num_server,\n",
    "    \"num_server_init\": num_server_init\n",
    "}\n",
    "\n",
    "# Set device\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Starting simulation at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Initialize the simulator\n",
    "simulator = CallCenterSimulator(\n",
    "    config=config,\n",
    "    num_paths=1000000,\n",
    "    device=device,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "simulator.run_until_time(target_time=17)\n",
    "\n",
    "# # Print all history records\n",
    "# print(\"=== Full History ===\")\n",
    "# for i, record in enumerate(simulator.history):\n",
    "#     print(f\"\\nRecord {i}:\")\n",
    "#     print(\"Time  :\", record[\"time\"].tolist())\n",
    "#     print(\"State :\", record[\"state\"].t().tolist())\n",
    "#     print(\"Reward:\", record[\"reward\"].tolist())\n",
    "\n",
    "# Print final state and time\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Terminal State:\", [f\"{x:.2f}\" for x in simulator.current_states.float().mean(dim=1).tolist()])\n",
    "print(\"Terminal Time:\", f\"{simulator.current_times.mean().item():.2f}\")\n",
    "\n",
    "print(\"Total Loss:\", f\"{simulator.total_reward.mean().item():.2f}\")  # Negative since rewards are costs\n",
    "print(\"Total Loss by Class:\", [f\"{x:.2f}\" for x in simulator.total_reward_by_class.mean(dim=1).tolist()])\n",
    "\n",
    "print(f\"Ending simulation at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchbsde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
